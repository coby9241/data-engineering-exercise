# Data Engineering Exercise

## Introduction

This is a simple data engineering exercise. Assume you are joining an e-commerce company ACME corporation as a new
data engineer. Upon joining, you realise that the team does not have any data team, and all operational reports
are generated by the engineering team. The engineering team has been so far generating the reports by querying the
backend ecommerce database directly. While this was still manageable in the past, as the company increased their footprint to
more countries and the number of orders grew, querying via the backend DB is no longer an option.

The team is now handling 500,000 orders per day, and what they have done is to push all order events into an event stream (in this case Kafka). 
There are also some other metadata tables inside PostgreSQL, and the aim is to build a reporting layer that can serve business needs and queries.
Business is interested in getting answers from the data to questions (but not limited to):
- number of orders placed per day
- how many orders did we fulfil per day (by fulfil it means DELIVERED, not just PLACED)
- fulfilment rate of each Distribution Center (DC) - fulfilment rate is number of fulfiled orders / total orders
- take home value per order across different entities
  - take home value per order: total price of orders - fees from payment method - shipping fee
- number of orders managed by per warehouse manager
- number of orders made per payment method
- payment method failure rate
- number of orders per day made by members in franchise vs non-franchise country

and so on.

## Task

Your task is to work with both the sample sql data, and the Kafka data provided, to try and build a reporting layer
that can answer questions that business is interested in. While the above business questions must be answered by the eventual reporting
layer that is built, the questions are not exhaustive and your solution should also be extensible to answer other questions business
may be interested in as well.

The output of the reporting layer should be a data source that can be easily connected to BI tools like Looker and Tableau to
create dashboards and gain insights that can help to answer the above questions, but you are not required to implement the BI dashboard etc..
You are expected to build a data transformation pipeline to achieve it, and the final submission should include any scripts/transformations
done to create the reporting layer. If any scheduling needs to be done for the scripts/transformation, you are not required to implement the scheduling, but you
will need to briefly describe outline how the scheduling will be done, at what intervals and with what tool (with a brief justification).

You are not restricted to any tool, nor any language to implement the pipeline. You are expected to read through the schema yourself
and decide how the data should be joined.

You will be given 1 week from the start of when this assignment is shared to complete the task. At the end of the assignment,
the following should be submitted:
- a zip file of your code (or a git repo), containing the scripts (python/SQL/etc.) used to construct the final reporting layer
from the Kafka and PostgreSQL data
- a written description of your solution outlining how you would run the jobs required to construct the reporting layer


## Grading Criteria

We will grade the assignment based on the following:
- correctness (45%): you are not required to present a fully working solution with all the pipelines running and scheduling enabled etc. However,
we will grade the submission based on what was submitted and decide if the solution is sufficient to answer the questions from business,
as well as if there's any logic errors in any of the code leading to the final reporting layer
- code style and quality (45%): we would evaluate the code submitted and see if its of an acceptable quality. SQL scripts if submitted
will be evaluated as well on style and quality. While we do not enforce that tests would be written, we believe strongly in testing, 
and having well written tests would be greatly favoured upon in the final evaluation of this component.
- documentation and design choices (10%): while design choices etc. is important, they should be evaluated in other rounds. The focus
of this task is on implementation skills, but we will still leave a small percentage for your documentation and design choices. That doesn't
mean you should not document at all, as sometimes your documentation is required for us to judge the correctness of the solution.

## Getting Started

### Pre-requisites

This task assumes you are using the following:
- a macbook
- have docker desktop installed

If you have either one of these not available, do let us know, and we can make adjustments if needed.

### Steps

We have provided a few things to help you get started:
- a docker compose file with dependencies such as Kafka and PostgreSQL
- some python script for producing order data to kafka
- Some init scripts for PostgreSQL

To install dependencies like pipenv and pyenv for running the producer script:
```shell
make deps
```

To install the python version and libraries. Note that if the pyenv already exists, you will be prompted to skip. Just press
`n` and it should work.
```shell
make install
```

To bring up the Kafka + PostgreSQL container
```shell
make up
```

To stop the containers, run:
```shell
make stop
```

:skull: If you need to purge data, run
```shell
make clean
```

To start the consumer and produce order events to Kafka using default values
```shell
make start
```

Note that the python script accepts two arguments:
```shell
python scripts/producer.py --loglevel INFO --interval 30
```
loglevel controls the logging verbosity while interval controls how many seconds before a new order event is published to Kafka.
Valid values for loglevel include: INFO, WARN, DEBUG etc. For 
